{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyQt5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mQtCore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QSettings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import mediapipe as mp\n",
    "from PyQt5.QtCore import QSettings\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('gesture_recognition_model.h5')\n",
    "\n",
    "# Initialize Mediapipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# State variables\n",
    "mode = 0\n",
    "\n",
    "def switch_mode():\n",
    "    global mode\n",
    "    mode += 1\n",
    "    print(f\"Switched to mode {mode}\")\n",
    "\n",
    "# Load settings\n",
    "settings = QSettings(\"MyApp\", \"MyAppSettings\")\n",
    "camera_str = settings.value(\"camera\", \"Camera 0\")\n",
    "\n",
    "# Extract the camera index from the string\n",
    "camera_index = int(camera_str.split()[-1])\n",
    "\n",
    "# Open the webcam with the camera index from settings\n",
    "cap = cv2.VideoCapture(camera_index)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open camera with index {camera_index}\")\n",
    "    sys.exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a later selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Prepare the landmarks for prediction\n",
    "            landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "            flattened_landmarks = np.array([coord for point in landmarks for coord in point]).reshape(1, -1)\n",
    "\n",
    "            # Predict the gesture\n",
    "            prediction = model.predict(flattened_landmarks)\n",
    "            predicted_mode = np.argmax(prediction)\n",
    "\n",
    "            cv2.putText(frame, f'Mode: {predicted_mode}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Gesture Recognition', frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('2'):\n",
    "        switch_mode()\n",
    "    elif key == ord('3'):\n",
    "        break\n",
    "\n",
    "# Release the resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
